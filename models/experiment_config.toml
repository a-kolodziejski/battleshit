# Configuration file for various experiments. It is read by experiments.py to perform the experiments specified by given paramteres.

#################################################################################

[experiment_1]
name = "Experiment_1"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "CartPole-v1"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_1.model]
model_kind = "SimpleFCN"
input_dim = 4
output_dim = 2
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_1.buffer]

[experiment_1.agent]
agent_kind = "preDQNAgentNoBatch"
bootstrap = "qlearning"
epsilon = 0.5
gamma = 0.99
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 100000
num_samples = 1 # preDQNAgentNoBatch updates NN after single intaraction with environment
test_freq = 10
num_trials = 10

#########################################################################

[experiment_2]
name = "Experiment_2"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "LunarLander-v3"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_2.model]
model_kind = "SimpleFCN"
input_dim = 8
output_dim = 4
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_2.buffer]

[experiment_2.agent]
agent_kind = "preDQNAgentNoBatch"
bootstrap = "qlearning"
epsilon = 0.5
gamma = 0.99
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 300000
num_samples = 1 # preDQNAgentNoBatch updates NN after single intaraction with environment
test_freq = 10
num_trials = 10


############################################################################
[experiment_3]
name = "Experiment_3"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "CartPole-v1"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_3.model]
model_kind = "SimpleFCN"
input_dim = 4
output_dim = 2
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_3.buffer]
buffer_kind = "SimpleBuffer"
buffer_size = 10000
buffer_min_capacity = 320


[experiment_3.agent]
agent_kind = "DQNAgent"
init_epsilon = 1
end_epsilon = 0.3
decay_ratio = 5000
tau = 0.01
gamma = 0.99
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 100000
batch_size = 32
steps_in_env = 10
update_freq = 15
test_freq = 10
num_trials = 10

############################################################################
[experiment_4]
name = "Experiment_4"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "CartPole-v1"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_4.model]
model_kind = "SimpleFCN"
input_dim = 4
output_dim = 2
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_4.buffer]
buffer_kind = "SimpleBuffer"
buffer_size = 10000
buffer_min_capacity = 320


[experiment_4.agent]
agent_kind = "DQNAgent"
init_epsilon = 1
end_epsilon = 0.3
decay_ratio = 10000
tau = 0.9
gamma = 0.99
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 100000
batch_size = 64
steps_in_env = 10
update_freq = 15
test_freq = 10
num_trials = 10

############################################################################
[experiment_5]
name = "Experiment_5"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "LunarLander-v3"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_5.model]
model_kind = "SimpleFCN"
input_dim = 8
output_dim = 4
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_5.buffer]
buffer_kind = "SimpleBuffer"
buffer_size = 20000
buffer_min_capacity = 320


[experiment_5.agent]
agent_kind = "DoubleDQNAgent"
init_epsilon = 1
end_epsilon = 0.3
decay_ratio = 10000
tau = 0.9
gamma = 0.99
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 300000
batch_size = 64
steps_in_env = 5
update_freq = 15
test_freq = 10
num_trials = 10


############################################################################
[experiment_6]
name = "Experiment_6"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "LunarLander-v3"
save_model = true
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_6.model]
model_kind = "DuellingFCN"
input_dim = 8
output_dim = 4
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_6.buffer]
buffer_kind = "SimpleBuffer"
buffer_size = 20000
buffer_min_capacity = 320


[experiment_6.agent]
agent_kind = "DoubleDQNAgent"
init_epsilon = 1
end_epsilon = 0.3
decay_ratio = 20000
tau = 0.1
gamma = 0.9
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 300000
batch_size = 64
steps_in_env = 1
update_freq = 1
test_freq = 50
num_trials = 10


############################################################################
[experiment_7]
name = "Experiment_7"
active = false
environment_type = "gym" # gym or custom. If custom, it means one of Battleship environments
environment = "LunarLander-v3"
save_model = false
save_model_path = "models/saved_models/"
save_graph = false
save_graph_path = "models/saved_graphs/"

[experiment_7.model]
model_kind = "DuellingFCN"
input_dim = 8
output_dim = 4
hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_7.buffer]
buffer_kind = "PrioritizedReplayBuffer"
buffer_size = 20000
buffer_min_capacity = 320
alpha = 0.6
beta = 0.4
beta_increment_per_sampling = 0.001
epsilon = 0.00001
rank_based = false


[experiment_7.agent]
agent_kind = "PERDoubleDQNAgent"
init_epsilon = 1
end_epsilon = 0.3
decay_ratio = 20000
tau = 0.1
gamma = 0.9
optimizer = "Adam"
learning_rate = 0.0005
num_steps = 10
batch_size = 32
steps_in_env = 5
update_freq = 10
test_freq = 50
num_trials = 10

############################################################################
[experiment_8]
name = "Experiment_8"
active = true
environment_type = "gym" 
environment = "CartPole-v1"
save_model = false
save_model_path = "models/saved_models/"
save_graph = true
save_graph_path = "models/saved_graphs/"

[experiment_8.model]
model_kind = "SimpleFCN"
actor_input_dim = 4
actor_output_dim = 2
critic_input_dim = 4
critic_output_dim = 1
actor_hidden_dims = [512, 128]
critic_hidden_dims = [512, 128]
hidden_activation = "ReLU"
output_activation = "Identity"

[experiment_8.buffer] # No buffer this time

[experiment_8.agent]
agent_kind = "REINFORCE"
gamma = 1.0
optimizer = "Adam"
learning_rate = 0.001
beta = 0
greedy = false
num_trajectories = 10 # Number of trajectories to average
num_updates = 150
test_freq = 10
num_trials = 10